{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avalanche Shipping Analytics Lab\n",
    "\n",
    "## Lab Overview\n",
    "Welcome to the Avalanche Shipping Analytics Lab! In this hands-on exercise, you'll apply the concepts from Module 2 to analyze real shipping data from Avalanche's logistics operations. Instead of customer reviews, you'll be working with shipping logs that contain critical information about order fulfillment, carrier performance, and delivery tracking.\n",
    "\n",
    "## Lab Scenario\n",
    "Avalanche's operations team has been collecting shipping data across multiple carriers and regions. They need insights into:\n",
    "- Carrier performance and delivery success rates\n",
    "- Geographic distribution of shipments\n",
    "- Shipping status trends over time\n",
    "- Potential bottlenecks in their logistics network\n",
    "\n",
    "Your task is to load this shipping data into Snowflake and prepare it for analysis using the same techniques you learned in the lessons.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "1. Navigate Snowsight to create databases, schemas, and stages\n",
    "2. Upload and verify data files in Snowflake\n",
    "3. Load data using both Pandas and Snowflake DataFrames\n",
    "4. Create and populate Snowflake tables from raw data\n",
    "5. Perform basic data exploration and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Data Description\n",
    "\n",
    "You'll be working with `shipping-logs.md` which contains:\n",
    "- **Order ID**: Unique identifier for each shipment\n",
    "- **Shipping Date**: When the order was shipped\n",
    "- **Carrier**: Logistics company handling the delivery\n",
    "- **Tracking Number**: Unique tracking identifier\n",
    "- **Latitude/Longitude**: Geographic coordinates\n",
    "- **Status**: Current delivery status (Delivered, In Transit, Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting Up Your Snowflake Environment\n",
    "\n",
    "### Manual Setup Steps (Complete these in Snowsight)\n",
    "\n",
    "1. **Create Database**\n",
    "   - Navigate to Snowsight (https://app.snowflake.com)\n",
    "   - Click \"Databases\" in the left menu\n",
    "   - Click \"+\" and select \"Create Database\"\n",
    "   - Name it `AVALANCHE_LOGISTICS_DB`\n",
    "\n",
    "2. **Create Schema**\n",
    "   - Click into your new database\n",
    "   - Go to the \"Schemas\" tab\n",
    "   - Click \"+\" to add a new schema\n",
    "   - Name it `SHIPPING_ANALYTICS`\n",
    "\n",
    "3. **Create Stage**\n",
    "   - In your database, go to the \"Stages\" tab\n",
    "   - Click \"+\" to create a new stage\n",
    "   - Choose \"Internal Named\" and name it `SHIPPING_STAGE`\n",
    "\n",
    "4. **Upload Data File**\n",
    "   - Save the shipping-logs.md file to your local machine\n",
    "   - In Snowsight, navigate to your stage\n",
    "   - Upload the shipping-logs.md file\n",
    "\n",
    "‚úÖ **Checkpoint**: Verify your database structure exists before proceeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Import Required Libraries\n",
    "\n",
    "Run this cell to import all the libraries you'll need for this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from snowflake.snowpark import Session\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Load and Explore the Shipping Data\n",
    "\n",
    "First, let's load the shipping logs data and explore its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the shipping logs file\n",
    "# Note: Update the file path to match where you saved the shipping-logs.md file\n",
    "file_path = 'shipping-logs.md'  # Update this path as needed\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    print(\"File loaded successfully!\")\n",
    "    print(f\"File content length: {len(content)} characters\")\n",
    "    print(\"\\nFirst 300 characters:\")\n",
    "    print(content[:300])\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    print(\"Please update the file_path variable to point to your shipping-logs.md file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Parse the Data\n",
    "\n",
    "The shipping logs are in a structured text format. Let's parse them into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the data (since it's in a structured text format)\n",
    "orders = []\n",
    "order_blocks = content.strip().split('\\n\\n')\n",
    "\n",
    "for block in order_blocks:\n",
    "    if block.strip():\n",
    "        order_data = {}\n",
    "        lines = block.strip().split('\\n')\n",
    "        for line in lines:\n",
    "            if ':' in line:\n",
    "                key, value = line.split(':', 1)\n",
    "                order_data[key.strip()] = value.strip()\n",
    "        if order_data:\n",
    "            orders.append(order_data)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(orders)\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Data Cleaning and Preparation\n",
    "\n",
    "Let's clean the data and prepare it for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and convert data types\n",
    "df['Order ID'] = df['Order ID'].astype(int)\n",
    "df['Shipping Date'] = pd.to_datetime(df['Shipping Date'])\n",
    "df['Latitude'] = df['Latitude'].astype(float)\n",
    "df['Longitude'] = df['Longitude'].astype(float)\n",
    "\n",
    "# Rename columns for easier SQL access (remove spaces)\n",
    "df.columns = [col.replace(' ', '_').lower() for col in df.columns]\n",
    "\n",
    "# Display cleaned data info\n",
    "print(\"Cleaned dataset info:\")\n",
    "print(df.info())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nFirst 5 rows of cleaned data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Initial Data Exploration\n",
    "\n",
    "Let's explore the data to understand what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== SHIPPING DATA OVERVIEW ===\")\n",
    "print(f\"Total shipments: {len(df)}\")\n",
    "print(f\"Date range: {df['shipping_date'].min()} to {df['shipping_date'].max()}\")\n",
    "print(f\"Number of unique carriers: {df['carrier'].nunique()}\")\n",
    "\n",
    "print(\"\\n=== CARRIER DISTRIBUTION ===\")\n",
    "print(df['carrier'].value_counts())\n",
    "\n",
    "print(\"\\n=== STATUS DISTRIBUTION ===\")\n",
    "print(df['status'].value_counts())\n",
    "\n",
    "print(\"\\n=== GEOGRAPHIC RANGE ===\")\n",
    "print(f\"Latitude range: {df['latitude'].min():.4f} to {df['latitude'].max():.4f}\")\n",
    "print(f\"Longitude range: {df['longitude'].min():.4f} to {df['longitude'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Connect to Snowflake\n",
    "\n",
    "‚ö†Ô∏è **Important**: Update the connection parameters with your actual Snowflake account details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowflake connection parameters\n",
    "# ‚ö†Ô∏è UPDATE THESE VALUES WITH YOUR ACTUAL SNOWFLAKE DETAILS\n",
    "connection_parameters = {\n",
    "    \"account\": \"your_account_identifier\",  # e.g., \"xy12345.us-east-1\"\n",
    "    \"user\": \"your_username\",              # Your Snowflake username\n",
    "    \"password\": \"your_password\",          # Your Snowflake password\n",
    "    \"role\": \"your_role\",                  # e.g., \"ACCOUNTADMIN\" or \"SYSADMIN\"\n",
    "    \"warehouse\": \"your_warehouse\",        # e.g., \"COMPUTE_WH\"\n",
    "    \"database\": \"AVALANCHE_LOGISTICS_DB\",\n",
    "    \"schema\": \"SHIPPING_ANALYTICS\"\n",
    "}\n",
    "\n",
    "# Create Snowflake session\n",
    "try:\n",
    "    session = Session.builder.configs(connection_parameters).create()\n",
    "    print(\"‚úÖ Connected to Snowflake successfully!\")\n",
    "    print(f\"Current database: {session.get_current_database()}\")\n",
    "    print(f\"Current schema: {session.get_current_schema()}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {str(e)}\")\n",
    "    print(\"Please check your connection parameters and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Create Snowflake Table\n",
    "\n",
    "Let's write the shipping data to a Snowflake table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrame to Snowflake DataFrame and write to table\n",
    "try:\n",
    "    snowflake_df = session.write_pandas(\n",
    "        df, \n",
    "        table_name=\"SHIPPING_LOGS\", \n",
    "        auto_create_table=True,\n",
    "        overwrite=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Shipping logs table created successfully!\")\n",
    "    print(f\"Table name: SHIPPING_LOGS\")\n",
    "    print(f\"Rows inserted: {len(df)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating table: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Verify the Upload\n",
    "\n",
    "Let's confirm our data loaded correctly into Snowflake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the table to verify data\n",
    "try:\n",
    "    # Count total rows\n",
    "    count_result = session.sql(\"SELECT COUNT(*) as row_count FROM SHIPPING_LOGS\").collect()\n",
    "    print(f\"Total rows in Snowflake table: {count_result[0]['ROW_COUNT']}\")\n",
    "    \n",
    "    # Get sample data\n",
    "    sample_result = session.sql(\"SELECT * FROM SHIPPING_LOGS LIMIT 5\").collect()\n",
    "    print(\"\\nSample data from Snowflake table:\")\n",
    "    for i, row in enumerate(sample_result, 1):\n",
    "        print(f\"Row {i}: Order {row['ORDER_ID']}, Carrier: {row['CARRIER']}, Status: {row['STATUS']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error querying table: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Basic Analytics\n",
    "\n",
    "Now let's perform some basic analysis on our shipping data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total shipments by status\n",
    "status_analysis = session.sql(\"\"\"\n",
    "    SELECT \n",
    "        status,\n",
    "        COUNT(*) as shipment_count,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n",
    "    FROM SHIPPING_LOGS\n",
    "    GROUP BY status\n",
    "    ORDER BY shipment_count DESC\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"=== SHIPMENT STATUS ANALYSIS ===\")\n",
    "for row in status_analysis:\n",
    "    print(f\"{row['STATUS']}: {row['SHIPMENT_COUNT']} shipments ({row['PERCENTAGE']}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Carrier Performance Analysis\n",
    "\n",
    "Let's analyze how well each carrier is performing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze carrier performance\n",
    "carrier_performance = session.sql(\"\"\"\n",
    "    SELECT \n",
    "        carrier,\n",
    "        COUNT(*) as total_shipments,\n",
    "        SUM(CASE WHEN status = 'Delivered' THEN 1 ELSE 0 END) as delivered_count,\n",
    "        SUM(CASE WHEN status = 'In Transit' THEN 1 ELSE 0 END) as in_transit_count,\n",
    "        SUM(CASE WHEN status = 'Processing' THEN 1 ELSE 0 END) as processing_count,\n",
    "        ROUND(delivered_count / total_shipments * 100, 2) as delivery_rate_percent\n",
    "    FROM SHIPPING_LOGS\n",
    "    GROUP BY carrier\n",
    "    ORDER BY total_shipments DESC\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"=== CARRIER PERFORMANCE ANALYSIS ===\")\n",
    "print(f\"{'Carrier':<25} {'Total':<8} {'Delivered':<10} {'In Transit':<10} {'Processing':<10} {'Delivery Rate':<12}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for row in carrier_performance:\n",
    "    print(f\"{row['CARRIER']:<25} {row['TOTAL_SHIPMENTS']:<8} {row['DELIVERED_COUNT']:<10} {row['IN_TRANSIT_COUNT']:<10} {row['PROCESSING_COUNT']:<10} {row['DELIVERY_RATE_PERCENT']:<12}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Geographic Distribution Analysis\n",
    "\n",
    "Let's analyze the geographic distribution of shipments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze geographic distribution by grouping nearby coordinates\n",
    "geo_analysis = session.sql(\"\"\"\n",
    "    SELECT \n",
    "        ROUND(latitude, 1) as lat_rounded,\n",
    "        ROUND(longitude, 1) as lon_rounded,\n",
    "        COUNT(*) as shipment_count,\n",
    "        COUNT(DISTINCT carrier) as carrier_count,\n",
    "        ROUND(AVG(CASE WHEN status = 'Delivered' THEN 1.0 ELSE 0.0 END) * 100, 1) as delivery_rate\n",
    "    FROM SHIPPING_LOGS\n",
    "    GROUP BY lat_rounded, lon_rounded\n",
    "    ORDER BY shipment_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"=== TOP SHIPPING LOCATIONS ===\")\n",
    "print(f\"{'Latitude':<10} {'Longitude':<12} {'Shipments':<10} {'Carriers':<10} {'Delivery Rate':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for row in geo_analysis:\n",
    "    print(f\"{row['LAT_ROUNDED']:<10} {row['LON_ROUNDED']:<12} {row['SHIPMENT_COUNT']:<10} {row['CARRIER_COUNT']:<10} {row['DELIVERY_RATE']:<12}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 13: Time-Based Analysis\n",
    "\n",
    "Let's analyze shipping trends over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze shipping trends over time\n",
    "time_analysis = session.sql(\"\"\"\n",
    "    SELECT \n",
    "        shipping_date,\n",
    "        COUNT(*) as daily_shipments,\n",
    "        COUNT(DISTINCT carrier) as carriers_used,\n",
    "        SUM(CASE WHEN status = 'Delivered' THEN 1 ELSE 0 END) as delivered,\n",
    "        SUM(CASE WHEN status = 'In Transit' THEN 1 ELSE 0 END) as in_transit,\n",
    "        SUM(CASE WHEN status = 'Processing' THEN 1 ELSE 0 END) as processing\n",
    "    FROM SHIPPING_LOGS\n",
    "    GROUP BY shipping_date\n",
    "    ORDER BY shipping_date\n",
    "    LIMIT 15\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"=== DAILY SHIPPING TRENDS (First 15 Days) ===\")\n",
    "print(f\"{'Date':<12} {'Total':<6} {'Carriers':<9} {'Delivered':<10} {'In Transit':<10} {'Processing':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for row in time_analysis:\n",
    "    date_str = str(row['SHIPPING_DATE'])[:10]  # Format date\n",
    "    print(f\"{date_str:<12} {row['DAILY_SHIPMENTS']:<6} {row['CARRIERS_USED']:<9} {row['DELIVERED']:<10} {row['IN_TRANSIT']:<10} {row['PROCESSING']:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 14: Advanced Analysis Questions\n",
    "\n",
    "Now it's your turn! Try to answer these business questions using SQL queries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Which carrier has the most consistent delivery performance?\n",
    "\n",
    "Write a query to find the carrier with the highest and most consistent delivery rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Calculate delivery rate by carrier and consider both rate and volume\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Are there geographic patterns in delivery success?\n",
    "\n",
    "Analyze if certain regions have better delivery success rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Group by geographic regions and compare delivery rates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: What's the relationship between shipping date and delivery status?\n",
    "\n",
    "Are more recent shipments more likely to be still \"In Transit\" or \"Processing\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Look at status distribution by date ranges\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Tracking Number Analysis\n",
    "\n",
    "Do different carriers use different tracking number patterns? Analyze the first few digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use string functions to analyze tracking number patterns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 15: Create a Summary Report\n",
    "\n",
    "Based on your analysis, create a summary report of key findings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a comprehensive summary report\n",
    "summary_query = \"\"\"\n",
    "SELECT \n",
    "    'Total Shipments' as metric,\n",
    "    COUNT(*)::STRING as value\n",
    "FROM SHIPPING_LOGS\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'Overall Delivery Rate' as metric,\n",
    "    ROUND(AVG(CASE WHEN status = 'Delivered' THEN 1.0 ELSE 0.0 END) * 100, 1) || '%' as value\n",
    "FROM SHIPPING_LOGS\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'Best Performing Carrier' as metric,\n",
    "    carrier as value\n",
    "FROM (\n",
    "    SELECT \n",
    "        carrier,\n",
    "        AVG(CASE WHEN status = 'Delivered' THEN 1.0 ELSE 0.0 END) as delivery_rate,\n",
    "        COUNT(*) as volume\n",
    "    FROM SHIPPING_LOGS\n",
    "    GROUP BY carrier\n",
    "    HAVING volume >= 3  -- Only carriers with sufficient volume\n",
    "    ORDER BY delivery_rate DESC\n",
    "    LIMIT 1\n",
    ")\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'Date Range' as metric,\n",
    "    MIN(shipping_date) || ' to ' || MAX(shipping_date) as value\n",
    "FROM SHIPPING_LOGS\n",
    "\"\"\"\n",
    "\n",
    "summary_results = session.sql(summary_query).collect()\n",
    "\n",
    "print(\"=== AVALANCHE SHIPPING ANALYTICS SUMMARY ===\")\n",
    "print(\"=\" * 50)\n",
    "for row in summary_results:\n",
    "    print(f\"{row['METRIC']}: {row['VALUE']}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 16: Cleanup and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the Snowflake session\n",
    "try:\n",
    "    session.close()\n",
    "    print(\"‚úÖ Snowflake session closed successfully\")\n",
    "except:\n",
    "    print(\"Session was already closed or not established\")\n",
    "\n",
    "print(\"\\nüéâ Lab completed successfully!\")\n",
    "print(\"\\nWhat you accomplished:\")\n",
    "print(\"‚úÖ Created Snowflake database structure\")\n",
    "print(\"‚úÖ Loaded and cleaned shipping data\")\n",
    "print(\"‚úÖ Created Snowflake table from raw data\")\n",
    "print(\"‚úÖ Performed comprehensive shipping analytics\")\n",
    "print(\"‚úÖ Generated business insights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Reflection Questions\n",
    "\n",
    "1. **Data Quality**: What data quality issues did you notice in the shipping logs? How would you address them in a production environment?\n",
    "\n",
    "2. **Scalability**: How would this process change if you had 1,000 shipping log files instead of one? What would you do differently?\n",
    "\n",
    "3. **Business Value**: What insights from this shipping data could help Avalanche improve their logistics operations?\n",
    "\n",
    "4. **Next Steps**: What additional data would you want to combine with this shipping data to create a more comprehensive logistics dashboard?\n",
    "\n",
    "## Bonus Challenges\n",
    "\n",
    "If you finish early, try these additional challenges:\n",
    "\n",
    "1. **Data Visualization**: Create a simple visualization showing carrier performance using matplotlib or seaborn\n",
    "2. **Automated Pipeline**: Write code that could automatically process new shipping log files as they arrive\n",
    "3. **Data Validation**: Create checks to ensure data quality before loading into Snowflake\n",
    "4. **Performance Optimization**: Research Snowflake best practices for table design and query performance\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Snowflake Documentation](https://docs.snowflake.com)\n",
    "- [Snowpark Python Documentation](https://docs.snowflake.com/en/developer-guide/snowpark/python/index.html)\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: Remember to replace placeholder